{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef85759",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62c6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from conllu import parse\n",
    "from conllu import models\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "015a2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6adeb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conllu_path = 'en_ewt-ud-train.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0850f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(conllu_path, 'r', encoding='utf-8') as f:\n",
    "    conllufile = f.read()\n",
    "sentences = parse(conllufile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b36de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_sentence = sentences[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8118cf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"They are targetting ambulances\", \"American snipers are shooting children and pregnant women\", and \"They are using cluster bombs against civilians\" is all you get to hear from him.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_sentence.metadata['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc08340",
   "metadata": {},
   "source": [
    "## Проверка на узлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd777e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_match_node(token: models.Token, node_pattern: dict) -> bool:\n",
    "    \"Проверяет соответствует ли данный токен заданному паттерну\"\n",
    "    \n",
    "    for feat in node_pattern:\n",
    "        if feat in token.keys():\n",
    "            if not re.match(node_pattern[feat], token[feat], re.I):\n",
    "                return False\n",
    "        elif token['feats']:\n",
    "            if feat in token['feats']:\n",
    "                if not re.match(node_pattern[feat], token['feats'][feat]): \n",
    "                    return False\n",
    "            elif feat == \"exclude\":\n",
    "                for ef in node_pattern[feat]:\n",
    "                    if ef in token['feats']:\n",
    "                        return False\n",
    "        else:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d433ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_suitable_tokens(token_list: models.TokenList, node_pattern: dict) -> list:\n",
    "    \"Возвращает список из всех токенов, подходящих под заданный паттерн\"\n",
    "    \n",
    "    suitable_tokens = []\n",
    "    for token in token_list:\n",
    "        if token_match_node(token, node_pattern) and isinstance(token['id'], int):\n",
    "            suitable_tokens.append(token['id'] - 1)\n",
    "    return suitable_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69c476b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_nodes(nodes: dict, sentence: models.TokenList) -> dict:\n",
    "    \"\"\"Возвращает словарь {node_name: [possible_tokens]}\n",
    "    или пустой словарь, если не все ноды найдены\"\"\"\n",
    "    \n",
    "    nodes_tokens = {}\n",
    "    for node in nodes:\n",
    "        sutable_tokens = search_suitable_tokens(sentence, nodes[node])\n",
    "        if sutable_tokens:\n",
    "            nodes_tokens[node] = sutable_tokens\n",
    "        else:\n",
    "            return {}\n",
    "    return nodes_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3806c",
   "metadata": {},
   "source": [
    "__Пример:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pattern = {\n",
    "    'N': \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91695ab",
   "metadata": {},
   "source": [
    "## Проверка на ограничения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd2e69",
   "metadata": {},
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "162c3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_deprels(token_list: models.TokenList) -> defaultdict:\n",
    "    \"Cоздает словарь вида {'relation': (head, dependent)} из всех отношений в предложении\"\n",
    "    \n",
    "    deprels = defaultdict(list)\n",
    "    for t in token_list:\n",
    "        if isinstance(t['head'], int) and isinstance(t['id'], int):\n",
    "            deprels[t['deprel']].append((t['head'] - 1, t['id'] - 1))\n",
    "    return deprels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b33bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_relations(rel_pattern: str, sent_rels: defaultdict):\n",
    "    \"\"\"Возвращает все названия отношений в предложении, \n",
    "    которые описываются заданной регуляркой\"\"\"\n",
    "    \n",
    "    rels = []\n",
    "    for rel in sent_rels:\n",
    "        if re.search(rel_pattern, rel):\n",
    "            rels.append(rel)\n",
    "    return rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "638bdfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_with_rel(rel_name: str, sent_rels: defaultdict, possible_pairs: Iterable[tuple]) -> set: \n",
    "    \"Возвращает множество из пар токенов, между которыми отношение rel_name\"\n",
    "    \n",
    "    if not rel_name in sent_rels:\n",
    "        return False\n",
    "    else:\n",
    "        return set(sent_rels[rel_name]).intersection(possible_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65ea4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relpattern_tokens(possible_pairs: Iterable[tuple], sentence:models.TokenList, rel_pattern: str) -> set:\n",
    "    \"Возвращает все пары токенов, которые попадают под заданный паттерн rel_pattern\"\n",
    "    \n",
    "    sent_rels = all_deprels(sentence)\n",
    "    all_suitable_rels = set()\n",
    "    for rel in pattern_relations(rel_pattern, sent_rels):\n",
    "        all_suitable_rels = all_suitable_rels | tokens_with_rel(rel, sent_rels, possible_pairs)\n",
    "    return all_suitable_rels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eae72d",
   "metadata": {},
   "source": [
    "### Linear distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "658914e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_distance(possible_tokens_pairs: Iterable[tuple], lindist: tuple) -> set:\n",
    "    \"Возвращает только те пары токенов, между которыми заданное расстояние\"\n",
    "    \n",
    "    suitable_tokens = set()\n",
    "    for pair in possible_tokens_pairs:\n",
    "        dist = pair[1] - pair[0]\n",
    "        if dist >= lindist[0] and dist <= lindist[1]:\n",
    "            suitable_tokens.add(pair)\n",
    "    return suitable_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b60df",
   "metadata": {},
   "source": [
    "### Совпадение/Несовпадение значений признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bc539de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_match_fconstraint(token_pair: tuple, sentence: models.TokenList, c_pattern: dict) -> bool:\n",
    "    \"\"\"Проверяет соответствует ли пара токенов ограничениям на признаки\"\"\"\n",
    "    \n",
    "    t1_feats = sentence[token_pair[0]]['feats']\n",
    "    t2_feats = sentence[token_pair[1]]['feats']\n",
    "    if t1_feats and t2_feats:\n",
    "        for c in c_pattern:\n",
    "            for f in c_pattern[c]:\n",
    "                if (f in t1_feats) and (f in t2_feats): \n",
    "                    if c == 'intersec':\n",
    "                        if t1_feats[f] != t2_feats[f]:\n",
    "                            return False\n",
    "                    elif c == 'disjoint':\n",
    "                        if t1_feats[f] == t2_feats[f]:\n",
    "                            return False\n",
    "                else:\n",
    "                    return False\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "778b6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_constraint(possible_token_pairs: Iterable[tuple], sentence: models.TokenList, constr_pattern: dict) -> set:\n",
    "    \"\"\"Ищет среди пар токенов такие, которые соответствуют ограничениям на признаки\"\"\"\n",
    "    \n",
    "    suitable_pairs = set()\n",
    "    for pair in possible_token_pairs:\n",
    "        if pair_match_fconstraint(pair, sentence, constr_pattern):\n",
    "            suitable_pairs.add(pair)\n",
    "    return suitable_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75852bf7",
   "metadata": {},
   "source": [
    "### Tree distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b408277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ca8a014",
   "metadata": {},
   "source": [
    "### Сопоставление со всеми ограничениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "af0b2c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_constraints(nodes_constraints: dict, nodes_tokens: dict, sentence: models.TokenList) -> bool:\n",
    "    \"\"\"Проверяет предложение на все ограничения\"\"\"\n",
    "    \n",
    "    for np in nodes_constraints:\n",
    "        suitable_pairs = list(product(nodes_tokens[np[0]], nodes_tokens[np[1]])) #всевозможные комбинации токенов для нодов\n",
    "        for constraint in nodes_constraints[np]:\n",
    "            \n",
    "            if constraint == 'deprels':\n",
    "                suitable_pairs = relpattern_tokens(suitable_pairs, sentence, nodes_constraints[np][constraint])\n",
    "            elif constraint == 'lindist':\n",
    "                suitable_pairs = linear_distance(suitable_pairs, nodes_constraints[np][constraint])\n",
    "            elif constraint == 'fconstraint':\n",
    "                suitable_pairs = feature_constraint(suitable_pairs, sentence, nodes_constraints[np][constraint])\n",
    "                \n",
    "            if not suitable_pairs:\n",
    "                return False \n",
    "            else:\n",
    "                # удаляем токены, которые не подошли\n",
    "                nodes_tokens[np[0]] = [p[0] for p in suitable_pairs]\n",
    "                nodes_tokens[np[1]] = [p[1] for p in suitable_pairs]\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6231380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentence(sentence: models.TokenList, nodes_pattern: dict, constraints: dict) -> bool:\n",
    "    \"Возвращает True, если предложение соответствует заданному паттерну\"\n",
    "    \n",
    "    found_nodes = find_all_nodes(nodes_pattern, sentence)\n",
    "    if not found_nodes:\n",
    "        return False\n",
    "    else:\n",
    "        if not match_constraints(constraints, found_nodes, sentence): \n",
    "            return False\n",
    "        else: \n",
    "            return True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465beaf8",
   "metadata": {},
   "source": [
    "## Примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fc12ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# примеры запросов\n",
    "patterns_names = ['passive with by Agent', 'all the', '2+amod', 'SOmatchnumber']\n",
    "node_patterns = [\n",
    "    {\n",
    "        'V': {},\n",
    "        'S': {},\n",
    "        'BY': {'lemma': '^by$'},\n",
    "        'N': {},\n",
    "    },\n",
    "    {\n",
    "        'A': {'lemma': '^all$'},\n",
    "        'T': {'lemma': '^the$'},\n",
    "    },\n",
    "    {\n",
    "        'N': {},\n",
    "        'M': {'upos': 'ADJ'},\n",
    "    },\n",
    "    {\n",
    "        'S': {},\n",
    "        'V': {},\n",
    "        'O': {}\n",
    "    },\n",
    "]\n",
    "constraints = [\n",
    "    {\n",
    "        ('V', 'S'): {'deprels': '^aux:pass$'},\n",
    "        ('V', 'N'): {'deprels': '^obl$'},\n",
    "        ('N', 'BY'): {'deprels': '^case$'},\n",
    "    },\n",
    "    {\n",
    "        ('A', 'T'): {'lindist': (1, 1)}\n",
    "    },\n",
    "    {\n",
    "        ('N', 'M'): {'deprels': '^amod$', 'lindist': (-inf, -2)}\n",
    "    },\n",
    "    {\n",
    "        ('V', 'S'): {'deprels': '^.subj$'},\n",
    "        ('V', 'O'): {'deprels': '^obj$'},\n",
    "        ('S', 'O'): {'fconstraint': {'intersec': 'Number'}}\n",
    "    }\n",
    "]\n",
    "tasks = [(patterns_names[i], node_patterns[i], constraints[i]) for i in range(len(patterns_names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40d27a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probing_dict(class_label: str, sentences: Iterable[models.TokenList],\n",
    "                nodes_pattern: dict, constraints: dict) -> dict:\n",
    "    \"\"\"Составляет словарь {название класса: список предложений}\"\"\"\n",
    "    \n",
    "    pd = defaultdict(list)\n",
    "    for sentence in sentences:\n",
    "        if filter_sentence(sentence, nodes_pattern, constraints):\n",
    "            pd[class_label].append(sentence.metadata['text'])\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69e9a6ba",
   "metadata": {},
   "source": [
    "task = tasks[0]\n",
    "probing_dict(task[0], sentences, task[1], task[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3ccf1",
   "metadata": {},
   "source": [
    "# Проблема\n",
    "Кратко: я умею последовательно проверять на отношения между парами нодов, но не могу проверять на __одновременные__ отношения между всеми нодами. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021350be",
   "metadata": {},
   "source": [
    "Например, есть предложение _\"Маша купила овощи, а мальчики купили самовар\"_. Мы хотим отфильтровать предложения по совпадению числа у подлежащего и сказуемого. Для этого будем использовать следующий паттерн:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20ceb5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SOmatchnumber',\n",
       " {'S': {}, 'V': {}, 'O': {}},\n",
       " {('V', 'S'): {'deprels': '^.subj$'},\n",
       "  ('V', 'O'): {'deprels': '^obj$'},\n",
       "  ('S', 'O'): {'fconstraint': {'intersec': 'Number'}}})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f64cea",
   "metadata": {},
   "source": [
    "Для начала делаем __проверку на узлы__. После нее получим следующие спиcки: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95e8e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3d1687a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': [0, 1, 2, 3, 4, 5, 6],\n",
       " 'V': [0, 1, 2, 3, 4, 5, 6],\n",
       " 'O': [0, 1, 2, 3, 4, 5, 6]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_nodes = {\n",
    "    'S': list(np.arange(7)),\n",
    "    'V': list(np.arange(7)),\n",
    "    'O': list(np.arange(7))\n",
    "}\n",
    "found_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f86b9f3",
   "metadata": {},
   "source": [
    "Потом идем проверять на ограничения. Для этого составляем декартово произведение для первой пары."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9b2a0cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vs = list(product(found_nodes['V'], found_nodes['S']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389edde",
   "metadata": {},
   "source": [
    "Проверяем на наличие nsubj между S и V. relpattern_tokens вернет следующее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1fef0d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 0), (5, 1)}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsubj = set([(1, 0), (5, 1)])\n",
    "nsubj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da170fdc",
   "metadata": {},
   "source": [
    "А также удалит из списков токенов S и V те, которые не подошли "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3608c093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': [0, 1], 'V': [1, 5], 'O': [0, 1, 2, 3, 4, 5, 6]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_nodes['V'] = [p[0] for p in nsubj]\n",
    "found_nodes['S'] = [p[1] for p in nsubj]\n",
    "found_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2049312",
   "metadata": {},
   "source": [
    "Повторяем для obj и S.Number=O.Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4a32742c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': [0, 1], 'V': [1, 5], 'O': [2, 6]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vo = list(product(found_nodes['V'], found_nodes['O']))\n",
    "obj = set([(1, 2), (5, 6)])\n",
    "found_nodes['V'] = [p[0] for p in obj]\n",
    "found_nodes['O'] = [p[1] for p in obj]\n",
    "found_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88770066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': [4, 0], 'V': [1, 5], 'O': [2, 6]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "so = list(product(found_nodes['S'], found_nodes['O']))\n",
    "match_number = set([(0, 6), (4, 2)])\n",
    "found_nodes['S'] = [p[0] for p in match_number]\n",
    "found_nodes['O'] = [p[1] for p in match_number]\n",
    "found_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b33a5ba",
   "metadata": {},
   "source": [
    "И как бы получается, что через мой фильтр это предложение прошло (ни разу не возвращалось пустое множество из пар), но при этом, это предложение не подходит. То есть надо как-то проверять одновременные отношения между токенами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe924a",
   "metadata": {},
   "source": [
    "Можно сделать:\n",
    "- делать произведение не списков пары нодов, а списков всех нодов (но это будет очень длинные списки, особенно если на сами ноды не наложены морфлогические ограничения и ими могут быть любые токены из предложения) &emsp;product(found_nodes['V'], found_nodes['O'], found_nodes['S'])\n",
    "- после моей проверки (когда уже в списках сильно меньше токенов) делать декартово произведение и снова проверять на те же условия (но опять проходиться по циклу, которых у меня и так много...) &emsp;[(4, 1, 2), (4, 1, 6), ..., (0, 5, 6)]\n",
    "- какой-то более умный способ (мне кажется что-то должно быть) \n",
    "- вообще поменять всю идею... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
