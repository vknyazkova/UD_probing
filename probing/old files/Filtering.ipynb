{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef85759",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from conllu import parse\n",
    "from conllu import models\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6adeb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conllu_path = 'en_ewt-ud-train.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c705c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # чтобы сверяться с grew-match\n",
    "# dev_path = 'en_ewt-ud-dev.conllu.txt'\n",
    "# train_path = 'en_ewt-ud-train.conllu'\n",
    "# test_path = 'en_ewt-ud-test.conllu.txt'\n",
    "\n",
    "# with open(dev_path, 'r', encoding='utf-8') as f:\n",
    "#     devfile = f.read()\n",
    "# dev = parse(devfile)\n",
    "\n",
    "# with open(train_path, 'r', encoding='utf-8') as f:\n",
    "#     trainfile = f.read()\n",
    "# train = parse(trainfile)\n",
    "\n",
    "# with open(test_path, 'r', encoding='utf-8') as f:\n",
    "#     testfile = f.read()\n",
    "# test = parse(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(conllu_path, 'r', encoding='utf-8') as f:\n",
    "    conllufile = f.read()\n",
    "    sentences = parse(conllufile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b36de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_sentence = sentences[23]\n",
    "some_sentence.metadata['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc08340",
   "metadata": {},
   "source": [
    "## Проверка на узлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd777e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_match_node(token: models.Token, node_pattern: dict) -> bool:\n",
    "    \"Проверяет соответствует ли данный токен заданному паттерну\"\n",
    "    \n",
    "    for feat in node_pattern:\n",
    "        if feat in token.keys():\n",
    "            if not re.match(node_pattern[feat], token[feat], re.I):\n",
    "                return False\n",
    "        elif token['feats']:\n",
    "            if feat in token['feats']:\n",
    "                if not re.match(node_pattern[feat], token['feats'][feat]): \n",
    "                    return False\n",
    "            elif feat == \"exclude\":\n",
    "                for ef in node_pattern[feat]:\n",
    "                    if ef in token['feats']:\n",
    "                        return False\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e933088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # пример для token_match_node\n",
    "\n",
    "# ttoken = sentences[23][6]\n",
    "# node_patterns = [{'upos': '^AUX$', 'Number': '^Plur$'}, # есть признаки, но не то значение\n",
    "#                 {'upos': '^AUX$', 'Number': '^Sing$'}, # подходит\n",
    "#                 {'NumType': '^.*$'}] # нет такого признака у токена\n",
    "\n",
    "# for np in node_patterns:\n",
    "#     print(np, token_match_node(ttoken, np))\n",
    "# ttoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(found_nodes, sentence):\n",
    "    \"\"\"Вывод предложения с выделением токенов, подошедших под паттерн\"\"\"\n",
    "    pretty_string = ''\n",
    "    all_suitable_tokens = [t for ts in found_nodes.values() for t in ts]\n",
    "    for token in sentence:\n",
    "        word = token['form'] + ' '\n",
    "        if isinstance(token['id'], int):\n",
    "            if token['id'] - 1 in all_suitable_tokens:\n",
    "                word = '\\033[1m' + token['form'] + '\\033[0m' + ' '\n",
    "        pretty_string += word\n",
    "    print(pretty_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_suitable_tokens(token_list: models.TokenList, node_pattern: dict) -> list:\n",
    "    \"\"\"Проходит по списку из токенов и возвращает список \n",
    "    токенов, который подошли под заданный паттерн\"\"\"\n",
    "    \n",
    "    suitable_tokens = []\n",
    "    for token in token_list:\n",
    "        if token_match_node(token, node_pattern) and isinstance(token['id'], int):\n",
    "            suitable_tokens.append(token['id'] - 1)\n",
    "    return suitable_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85aa174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for pretty_print() and search_suitable_tokens()\n",
    "# print(some_sentence.serialize())\n",
    "# print(search_suitable_tokens(some_sentence, {'Number': 'Sing'}))\n",
    "# pretty_print({'N': search_suitable_tokens(some_sentence, {'Number': 'Sing'})}, some_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c476b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_nodes(nodes: dict, sentence: models.TokenList):\n",
    "    \"\"\"Ищет подходящие токены для каждого нода в паттерне.\n",
    "    Возвращает словарь {node_name: [possible_tokens]}\n",
    "    или пустой словарь, если не все ноды найдены\"\"\"\n",
    "    \n",
    "    nodes_tokens = {}\n",
    "    for node in nodes:\n",
    "        sutable_tokens = search_suitable_tokens(sentence, nodes[node])\n",
    "        if sutable_tokens:\n",
    "            nodes_tokens[node] = sutable_tokens\n",
    "        else:\n",
    "            return False #changed {} to False\n",
    "    return nodes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6797f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #пример \n",
    "# node_pattern = {\n",
    "#     'N': {'NumType': '^Card$'},\n",
    "#     'M': {'upos': '^aux$'},\n",
    "# }\n",
    "# for sentence in sentences:\n",
    "#     if find_all_nodes(node_pattern, sentence):\n",
    "#         pretty_print(find_all_nodes(node_pattern, sentence), sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91695ab",
   "metadata": {},
   "source": [
    "## Проверка на ограничения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd2e69",
   "metadata": {},
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_deprels(token_list: models.TokenList) -> defaultdict:\n",
    "    \"\"\"Cоздает словарь вида {'relation': (head, dependent)} из всех отношений в предложении\"\"\"\n",
    "    \n",
    "    deprels = defaultdict(list)\n",
    "    for t in token_list:\n",
    "        if isinstance(t['head'], int) and isinstance(t['id'], int):\n",
    "            deprels[t['deprel']].append((t['head'] - 1, t['id'] - 1))\n",
    "    return deprels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #пример\n",
    "# from pprint import pprint\n",
    "\n",
    "# print(some_sentence.metadata['text'])\n",
    "# pprint(all_deprels(some_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_relations(rel_pattern: str, sent_rels: defaultdict):\n",
    "    \"\"\"Возвращает все названия отношений в предложении, \n",
    "    которые описываются заданной регуляркой\"\"\"\n",
    "    \n",
    "    rels = []\n",
    "    for rel in sent_rels:\n",
    "        if re.search(rel_pattern, rel):\n",
    "            rels.append(rel)\n",
    "    return rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # пример\n",
    "# p = r'mod$'\n",
    "# pattern_relations(p, all_deprels(some_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bdfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_with_rel(rel_name: str, sent_rels: defaultdict, possible_pairs: Iterable[tuple]) -> set: \n",
    "    \"\"\"Выбирает из пар токенов те, между которыми отношение rel_name\"\"\"\n",
    "    \n",
    "    if not rel_name in sent_rels:\n",
    "        return False\n",
    "    else:\n",
    "        return set(sent_rels[rel_name]).intersection(possible_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a747a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # пример\n",
    "# ex_rel_name = 'advmod'\n",
    "# ex_sent_rels = all_deprels(some_sentence)\n",
    "# ex_possible_pairs = list(product(list(range(0, 30)), list(range(0, 30))))\n",
    "# tokens_with_rel(ex_rel_name, ex_sent_rels, ex_possible_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relpattern_tokens(possible_pairs: Iterable[tuple], sentence:models.TokenList, rel_pattern: str) -> set:\n",
    "    \"\"\"Ищет среди всех возможных пар токенов те, между которыми отношения,\n",
    "    описываемые заданным паттерном rel_pattern\"\"\"\n",
    "    \n",
    "    sent_rels = all_deprels(sentence)\n",
    "    all_suitable_rels = set()\n",
    "    for rel in pattern_relations(rel_pattern, sent_rels):\n",
    "        all_suitable_rels = all_suitable_rels | tokens_with_rel(rel, sent_rels, possible_pairs)\n",
    "    return all_suitable_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example\n",
    "# ex_rel_pattern = r'mod$'\n",
    "# ex_possible_pairs = list(product(list(range(0, 30)), list(range(0, 30))))\n",
    "# relpattern_tokens(ex_possible_pairs, some_sentence, ex_rel_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eae72d",
   "metadata": {},
   "source": [
    "### Linear distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658914e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_distance(possible_tokens_pairs: Iterable[tuple], lindist: tuple) -> set:\n",
    "    \"\"\"Ищет среди данных пар токенов те, между которыми заданное расстояние.\n",
    "    lindist = tuple(min_distance, max_distance)\"\"\"\n",
    "    \n",
    "    suitable_tokens = set()\n",
    "    for pair in possible_tokens_pairs:\n",
    "        dist = pair[1] - pair[0]\n",
    "        if dist >= lindist[0] and dist <= lindist[1]:\n",
    "            suitable_tokens.add(pair)\n",
    "    return suitable_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example\n",
    "# ex_tokens_pairs = [(2, 3), (4, 8), (4, 5), (2, 7)]\n",
    "# linear_distance(ex_tokens_pairs, (1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b60df",
   "metadata": {},
   "source": [
    "### Совпадение/Несовпадение значений признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc539de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_match_fconstraint(token_pair: tuple, sentence: models.TokenList, c_pattern: dict) -> bool:\n",
    "    \"\"\"Проверяет соответствует ли пара токенов ограничениям на признаки\"\"\"\n",
    "    \n",
    "    t1_feats = sentence[token_pair[0]]['feats']\n",
    "    t2_feats = sentence[token_pair[1]]['feats']\n",
    "    if t1_feats and t2_feats:\n",
    "        for c in c_pattern:\n",
    "            for f in c_pattern[c]:\n",
    "                if (f in t1_feats) and (f in t2_feats): \n",
    "                    if c == 'intersec':\n",
    "                        if t1_feats[f] != t2_feats[f]:\n",
    "                            return False\n",
    "                    elif c == 'disjoint':\n",
    "                        if t1_feats[f] == t2_feats[f]:\n",
    "                            \n",
    "                            return False\n",
    "                else:\n",
    "                    return False\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda6331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # пример\n",
    "# constr = {'intersec': ['Number']}\n",
    "# print(some_sentence[20], some_sentence[25], some_sentence[27])\n",
    "# pair1, pair2 = (27, 25), (20, 25)\n",
    "# pair_match_fconstraint(pair1, some_sentence, constr), pair_match_fconstraint(pair2, some_sentence, constr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_constraint(possible_token_pairs: Iterable[tuple], sentence: models.TokenList, constr_pattern: dict) -> set:\n",
    "    \"\"\"Ищет среди данных пар токенов такие, которые соответствуют ограничениям на признаки\"\"\"\n",
    "    \n",
    "    suitable_pairs = set()\n",
    "    for pair in possible_token_pairs:\n",
    "        if pair_match_fconstraint(pair, sentence, constr_pattern):\n",
    "            suitable_pairs.add(pair)\n",
    "    return suitable_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8a014",
   "metadata": {},
   "source": [
    "### Сопоставление со всеми ограничениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b2c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_constraints(nodes_constraints: dict, nodes_tokens: dict, sentence: models.TokenList) -> bool:\n",
    "    \"\"\"Для каждой пары нодов из ограничений отбирает те пары токенов, которые соответствуют всем ограничениям.\"\"\"\n",
    "    \n",
    "    for np in nodes_constraints:\n",
    "        suitable_pairs = list(product(nodes_tokens[np[0]], nodes_tokens[np[1]])) #всевозможные комбинации токенов для нодов\n",
    "        for constraint in nodes_constraints[np]:\n",
    "            if constraint == 'deprels':\n",
    "                suitable_pairs = relpattern_tokens(suitable_pairs, sentence, nodes_constraints[np][constraint])\n",
    "            elif constraint == 'lindist':\n",
    "                suitable_pairs = linear_distance(suitable_pairs, nodes_constraints[np][constraint])\n",
    "            elif constraint == 'fconstraint':\n",
    "                suitable_pairs = feature_constraint(suitable_pairs, sentence, nodes_constraints[np][constraint])\n",
    "            else:\n",
    "                raise ValueError('wrong constraint type')\n",
    "            \n",
    "            if not suitable_pairs:\n",
    "                return False \n",
    "            else:\n",
    "                #print('before:', nodes_tokens)\n",
    "                # удаляем токены, которые не подошли\n",
    "                nodes_tokens[np[0]] = list(set([p[0] for p in suitable_pairs]))\n",
    "                nodes_tokens[np[1]] = list(set([p[1] for p in suitable_pairs]))\n",
    "                #print('after:', nodes_tokens)\n",
    "    return nodes_tokens #changed from True to nodes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentence(sentence: models.TokenList, nodes_pattern: dict, constraints: dict) -> bool:\n",
    "    \"\"\"Проверяет предложение на соответствие паттерну. Если соответствует паттерну,\n",
    "    возвращает для каждого нода список из  подошедших токенов\"\"\"\n",
    "    \n",
    "    found_nodes = find_all_nodes(nodes_pattern, sentence) \n",
    "    if not found_nodes:\n",
    "        return False\n",
    "    else:\n",
    "        nodes_suitable_tokens = match_constraints(constraints, found_nodes, sentence)\n",
    "        if not nodes_suitable_tokens: \n",
    "            return False\n",
    "        else: \n",
    "            return nodes_suitable_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9132633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example \n",
    "n_pattern = {\n",
    "    'N': {'NumType': '^Card$'}, \n",
    "    'M': {}\n",
    "}\n",
    "r_pattern = {('M', 'N'): {'deprels': '^nummod$'}}\n",
    "print(sentences[2].metadata['text'])\n",
    "filter_sentence(sentences[2], n_pattern, r_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe22994",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    tokens = filter_sentence(sentence, n_pattern, r_pattern)\n",
    "    if tokens:\n",
    "        pretty_print(tokens, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea5879e",
   "metadata": {},
   "source": [
    "## Примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc12ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# примеры запросов\n",
    "patterns_names = ['passive with by Agent', 'all the', '2+amod', 'SOmatchnumber']\n",
    "node_patterns = [\n",
    "    {\n",
    "        'V': {},\n",
    "        'S': {},\n",
    "        'BY': {'lemma': '^by$'},\n",
    "        'N': {},\n",
    "    },\n",
    "    {\n",
    "        'A': {'lemma': '^all$'},\n",
    "        'T': {'lemma': '^the$'},\n",
    "    },\n",
    "    {\n",
    "        'N': {},\n",
    "        'M': {'upos': 'ADJ'},\n",
    "    },\n",
    "    {\n",
    "        'S': {},\n",
    "        'V': {},\n",
    "        'O': {}\n",
    "    },\n",
    "]\n",
    "constraints = [\n",
    "    {\n",
    "        ('V', 'S'): {'deprels': '^aux:pass$'},\n",
    "        ('V', 'N'): {'deprels': '^obl$'},\n",
    "        ('N', 'BY'): {'deprels': '^case$'},\n",
    "    },\n",
    "    {\n",
    "        ('A', 'T'): {'lindist': (1, 1)}\n",
    "    },\n",
    "    {\n",
    "        ('N', 'M'): {'deprels': '^amod$', 'lindist': (-inf, -2)}\n",
    "    },\n",
    "    {\n",
    "        ('V', 'S'): {'deprels': '^.subj$'},\n",
    "        ('V', 'O'): {'deprels': '^obj$'},\n",
    "        ('S', 'O'): {'fconstraint': {'intersec': 'Number'}}\n",
    "    }\n",
    "]\n",
    "tasks = [(patterns_names[i], node_patterns[i], constraints[i]) for i in range(len(patterns_names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d27a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probing_dict(class_label: str, sentences: Iterable[models.TokenList],\n",
    "                nodes_pattern: dict, constraints: dict) -> dict:\n",
    "    \"\"\"Составляет словарь {название класса: список предложений}\"\"\"\n",
    "    \n",
    "    pd = defaultdict(list)\n",
    "    for sentence in sentences:\n",
    "        if filter_sentence(sentence, nodes_pattern, constraints):\n",
    "            pd[class_label].append(sentence.metadata['text'])\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks[0]\n",
    "probing_dict(task[0], sentences, task[1], task[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    tokens = filter_sentence(sentence, task[1], task[2])\n",
    "    if tokens:\n",
    "        pretty_print(tokens, sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
